pipeline {
    agent any

    environment {
        DOCKER_REGISTRY = 'docker.io'
        DOCKER_REPO = '5unnysunny'
        KUBE_NAMESPACE = 'healthcare'
        MODEL_NAME = '5unnySunny/medical-flan-t5-small-log-summarizer'
        ES_HOST = 'http://elasticsearch:9200'
        MAX_TRAINING_SAMPLES = '50'  // Limit samples for faster training
    }

    triggers {
        // Run every 5 minutes for testing (change to 'H */6 * * *' for production)
        cron('H */6 * * *')
    }

    options {
        buildDiscarder(logRotator(numToKeepStr: '5'))
        timeout(time: 120, unit: 'MINUTES')
        timestamps()
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
                script {
                    env.MODEL_VERSION = sh(returnStdout: true, script: 'date +%Y%m%d%H%M%S').trim()
                }
            }
        }

        stage('Collect Training Data') {
            steps {
                script {
                    // Query medical logs from Elasticsearch via port-forward
                    sh """
                        mkdir -p training_data

                        # Start port-forward to Elasticsearch in background
                        kubectl port-forward service/elasticsearch 9200:9200 -n ${KUBE_NAMESPACE} &
                        PF_PID=\$!
                        sleep 3

                        # Fetch medical vitals data
                        curl -s 'http://localhost:9200/medical-vitals-*/_search?size=500' \\
                            -H 'Content-Type: application/json' \\
                            -d '{"query": {"range": {"@timestamp": {"gte": "now-7d"}}}}' \\
                            > training_data/vitals.json || echo '{"hits":{"hits":[]}}' > training_data/vitals.json

                        # Fetch medical alerts data
                        curl -s 'http://localhost:9200/medical-alerts-*/_search?size=10000' \\
                            -H 'Content-Type: application/json' \\
                            -d '{"query": {"range": {"@timestamp": {"gte": "now-7d"}}}}' \\
                            > training_data/alerts.json || true

                        # Fetch medical summaries for reference
                        curl -s 'http://localhost:9200/medical-summaries-*/_search?size=5000' \\
                            -H 'Content-Type: application/json' \\
                            -d '{"query": {"range": {"@timestamp": {"gte": "now-30d"}}}}' \\
                            > training_data/summaries.json || true

                        # Kill port-forward
                        kill \$PF_PID 2>/dev/null || true

                        # Show collected data
                        echo "Collected vitals: \$(cat training_data/vitals.json | python3 -c 'import json,sys; print(len(json.load(sys.stdin).get(\"hits\",{}).get(\"hits\",[])))'  2>/dev/null || echo 0) records"
                    """
                }
            }
        }

        stage('Prepare Dataset') {
            steps {
                dir('backend/summarizer-service') {
                    sh '''
                        pip3 install -r requirements.txt

                        # Create training script matching the original training format
                        cat > prepare_dataset.py << 'EOF'
import json
import os
import random

def generate_alerts(v):
    """Generate alerts based on vitals - matches original training logic."""
    alerts = []

    if v.get("spo2", 100) < 90:
        alerts.append("severe hypoxia")
    elif v.get("spo2", 100) < 94:
        alerts.append("low oxygen saturation")

    if v.get("hr", 70) > 140:
        alerts.append("tachycardia")
    elif v.get("hr", 70) < 50:
        alerts.append("bradycardia")

    if v.get("temp", 37) > 39.0:
        alerts.append("high fever")
    elif v.get("temp", 37) > 38.0:
        alerts.append("fever")

    bp_sys = v.get("bp_systolic", v.get("systolic", 120))
    bp_dia = v.get("bp_diastolic", v.get("diastolic", 80))
    if bp_sys > 180 or bp_dia > 120:
        alerts.append("hypertensive crisis")

    if v.get("resp", v.get("respiratory_rate", 16)) > 28:
        alerts.append("rapid breathing")
    elif v.get("resp", v.get("respiratory_rate", 16)) < 12:
        alerts.append("slow respiration")

    return alerts

def generate_summary(patient_id, conditions):
    """Generate summary text - matches original training logic."""
    if not conditions:
        return (
            f"Patient {patient_id} remained stable with no critical abnormalities "
            f"detected during the monitoring period."
        )

    conditions = list(set(conditions))
    issues = ", ".join(conditions)

    return (
        f"Patient {patient_id} experienced {issues} during the monitoring period "
        f"and requires continued medical observation."
    )

def prepare_training_data():
    """Prepare training data from collected medical vitals."""
    training_pairs = []

    vitals_file = '../../training_data/vitals.json'

    if not os.path.exists(vitals_file):
        print("No vitals data found")
        return 0

    with open(vitals_file, 'r') as f:
        vitals_data = json.load(f)

    hits = vitals_data.get('hits', {}).get('hits', [])
    print(f"Found {len(hits)} vitals records")

    # Group vitals by patient_id
    patient_vitals = {}
    for hit in hits:
        source = hit.get('_source', {})
        patient_id = source.get('patient_id', 'Unknown')

        if patient_id not in patient_vitals:
            patient_vitals[patient_id] = []
        patient_vitals[patient_id].append(source)

    # Create training pairs - group 5 vitals readings per training sample
    for patient_id, vitals_list in patient_vitals.items():
        # Process in groups of 5
        for i in range(0, len(vitals_list), 5):
            group = vitals_list[i:i+5]
            if len(group) < 2:  # Need at least 2 readings
                continue

            # Build input text (multiple vitals readings)
            logs = []
            all_alerts = []
            for v in group:
                hr = v.get('heart_rate', v.get('hr', 72))
                spo2 = v.get('spo2', v.get('oxygen_saturation', 98))
                temp = v.get('temperature', v.get('temp', 37.0))
                bp_sys = v.get('bp_systolic', v.get('systolic', 120))
                bp_dia = v.get('bp_diastolic', v.get('diastolic', 80))
                resp = v.get('respiratory_rate', v.get('resp', 16))

                log = f"Patient {patient_id}: HR={hr}, SpO2={spo2}, Temp={temp}, BP={bp_sys}/{bp_dia}, Resp={resp}."
                logs.append(log)

                # Generate alerts for this reading
                alerts = generate_alerts({
                    'hr': hr, 'spo2': spo2, 'temp': temp,
                    'bp_systolic': bp_sys, 'bp_diastolic': bp_dia,
                    'resp': resp
                })
                all_alerts.extend(alerts)

            # Build output text (summary)
            summary = generate_summary(patient_id, all_alerts)

            training_pairs.append({
                'input': ' '.join(logs),
                'summary': summary
            })

    # Save prepared dataset
    os.makedirs('training_output', exist_ok=True)
    with open('training_output/train_dataset.json', 'w') as f:
        json.dump(training_pairs, f, indent=2)

    print(f"Prepared {len(training_pairs)} training examples")
    return len(training_pairs)

if __name__ == '__main__':
    prepare_training_data()
EOF
                        python3 prepare_dataset.py
                    '''
                }
            }
        }

        stage('Fine-tune Model') {
            steps {
                withCredentials([string(credentialsId: 'huggingface-token', variable: 'HF_TOKEN')]) {
                    dir('backend/summarizer-service') {
                        sh '''
                            pip3 install -r requirements.txt datasets accelerate huggingface_hub -q

                            export MODEL_NAME="${MODEL_NAME}"
                            export NUM_EPOCHS=1
                            export HF_TOKEN="${HF_TOKEN}"

                            python3 finetune.py || echo "Fine-tuning skipped or failed"
                        '''
                    }
                }
            }
        }

        stage('Restart Summarizer Service') {
            steps {
                script {
                    // Restart pods to pick up the updated model from HuggingFace
                    sh """
                        kubectl rollout restart deployment/summarizer-service -n ${KUBE_NAMESPACE}

                        # Wait for rollout (non-blocking - don't fail pipeline on timeout)
                        kubectl rollout status deployment/summarizer-service \\
                            -n ${KUBE_NAMESPACE} \\
                            --timeout=120s || echo "Rollout still in progress (may need more memory)"
                    """
                }
            }
        }


        stage('Verify New Model') {
            steps {
                script {
                    // Verify the new model is serving (non-blocking)
                    sh """
                        # Wait for pods to be ready
                        sleep 30

                        # Get a running pod (not pending)
                        RUNNING_POD=\$(kubectl get pods -n ${KUBE_NAMESPACE} -l app=summarizer-service --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                        if [ -n "\$RUNNING_POD" ]; then
                            echo "Checking pod: \$RUNNING_POD"
                            kubectl exec -n ${KUBE_NAMESPACE} \$RUNNING_POD -- curl -s http://localhost:8003/health || echo "Health check failed"
                            echo "Model verification attempted on \$RUNNING_POD"
                        else
                            echo "No running summarizer pods found"
                        fi
                    """
                }
            }
        }

        stage('Log Deployment') {
            steps {
                script {
                    // Log deployment to Elasticsearch via port-forward
                    sh """
                        # Start port-forward
                        kubectl port-forward service/elasticsearch 9200:9200 -n ${KUBE_NAMESPACE} &
                        PF_PID=\$!
                        sleep 3

                        INDEX_DATE=\$(date +%Y.%m.%d)
                        TIMESTAMP=\$(date -Iseconds)

                        curl -X POST "http://localhost:9200/system-deployment-\${INDEX_DATE}/_doc" \\
                            -H 'Content-Type: application/json' \\
                            -d "{
                                \\"@timestamp\\": \\"\${TIMESTAMP}\\",
                                \\"event\\": \\"model_retrain\\",
                                \\"model_version\\": \\"${MODEL_VERSION}\\",
                                \\"status\\": \\"success\\",
                                \\"service\\": \\"summarizer-service\\"
                            }" || echo "Failed to log deployment"

                        kill \$PF_PID 2>/dev/null || true
                    """
                }
            }
        }
    }

    post {
        success {
            echo "Model retrained and deployed successfully: ${MODEL_VERSION}"
            slackSend(color: 'good', message: "Summarizer model ${MODEL_VERSION} retrained and deployed!")
        }
        failure {
            echo 'Retraining pipeline failed!'
            slackSend(color: 'danger', message: "Summarizer model retraining failed!")
        }
        always {
            cleanWs()
        }
    }
}
